## Phase 6: Advanced Features & Production Polish (Hard)

### Step 14: Smart Probability Regularization
```python
# shl/models/regularization.py
import numpy as np
from scipy import stats
import structlog

logger = structlog.get_logger()

class ProbabilityRegularizer:
    def __init__(self, config: Dict):
        self.config = config
        self.regularization_history = []
        
    def regularize_probabilities(self, 
                               probs: Tuple[float, float, float],
                               context: Dict = None) -> Tuple[float, float, float]:
        """Apply sophisticated regularization to extreme probabilities"""
        
        # Track original for analysis
        original = probs
        
        # Apply multiple regularization techniques
        probs = self._apply_extremity_shrinkage(probs)
        probs = self._apply_bayesian_regularization(probs, context)
        probs = self._apply_kelly_criterion_adjustment(probs, context)
        
        # Ensure valid probabilities
        probs = self._normalize_probabilities(probs)
        
        # Log regularization effect
        self._log_regularization(original, probs, context)
        
        return probs
    
    def _apply_extremity_shrinkage(self, probs: Tuple[float, float, float], 
                                  threshold: float = 0.7) -> Tuple[float, float, float]:
        """Shrink extreme probabilities toward mean"""
        regularized = list(probs)
        
        for i, p in enumerate(probs):
            if p > threshold:
                # Logarithmic shrinkage for extreme values
                excess = p - threshold
                shrinkage_factor = 1 - np.log(1 + excess) / 10
                regularized[i] = threshold + excess * shrinkage_factor
                
                logger.debug(f"Shrunk probability from {p:.3f} to {regularized[i]:.3f}")
        
        return tuple(regularized)
    
    def _apply_bayesian_regularization(self, probs: Tuple[float, float, float],
                                     context: Dict = None) -> Tuple[float, float, float]:
        """Bayesian shrinkage toward prior"""
        if context is None:
            context = {}
        
        # Define prior based on historical league data
        prior = context.get('league_prior', (0.45, 0.27, 0.28))  # Home/Draw/Away
        
        # Shrinkage strength based on sample size
        n_games = context.get('team_games_played', 10)
        shrinkage_strength = 1 / (1 + n_games / 10)  # Less shrinkage with more data
        
        # Apply shrinkage
        regularized = tuple(
            (1 - shrinkage_strength) * p + shrinkage_strength * prior_p
            for p, prior_p in zip(probs, prior)
        )
        
        return regularized
    
    def _apply_kelly_criterion_adjustment(self, probs: Tuple[float, float, float],
                                        context: Dict = None) -> Tuple[float, float, float]:
        """Adjust probabilities to prevent overconfident betting"""
        if context is None or 'odds' not in context:
            return probs
        
        # Kelly criterion suggests reducing confidence for betting safety
        kelly_fraction = 0.25  # Conservative Kelly
        
        regularized = []
        for i, p in enumerate(probs):
            if p > 0.5:  # Only adjust favorites
                # Simulate Kelly adjustment
                edge = p - (1 / context['odds'][i])
                if edge > 0:
                    # Reduce probability to imply more conservative betting
                    adjusted_p = p - (edge * (1 - kelly_fraction))
                    regularized.append(max(adjusted_p, 0.5))
                else:
                    regularized.append(p)
            else:
                regularized.append(p)
        
        return tuple(regularized)
    
    def _normalize_probabilities(self, probs: Tuple[float, float, float]) -> Tuple[float, float, float]:
        """Ensure probabilities sum to 1"""
        total = sum(probs)
        if abs(total - 1.0) > 0.001:
            logger.warning(f"Normalizing probabilities, sum was {total:.3f}")
            return tuple(p / total for p in probs)
        return probs
    
    def _log_regularization(self, original: Tuple, regularized: Tuple, context: Dict):
        """Track regularization effects"""
        max_change = max(abs(o - r) for o, r in zip(original, regularized))
        
        if max_change > 0.05:  # Significant change
            logger.info(f"Regularization applied: {original} -> {regularized}")
            
        self.regularization_history.append({
            'timestamp': pd.Timestamp.now(),
            'original': original,
            'regularized': regularized,
            'max_change': max_change,
            'context': context
        })
    
    def analyze_regularization_impact(self) -> pd.DataFrame:
        """Analyze historical regularization effects"""
        if not self.regularization_history:
            return pd.DataFrame()
        
        data = []
        for record in self.regularization_history:
            orig = record['original']
            reg = record['regularized']
            
            data.append({
                'timestamp': record['timestamp'],
                'max_original_prob': max(orig),
                'max_regularized_prob': max(reg),
                'total_change': sum(abs(o - r) for o, r in zip(orig, reg)),
                'max_change': record['max_change']
            })
        
        df = pd.DataFrame(data)
        
        # Summary statistics
        logger.info(f"Average total change: {df['total_change'].mean():.3f}")
        logger.info(f"Extreme probs (>0.7) regularized: "
                   f"{(df['max_original_prob'] > 0.7).sum()}")
        
        return df
```
**Win**: Sophisticated probability adjustment preventing overconfidence

### Step 15: Comprehensive Evaluation Framework
```python
# shl/evaluation/comprehensive_metrics.py
import numpy as np
from sklearn.calibration import calibration_curve
from typing import List, Dict, Tuple
import structlog

logger = structlog.get_logger()

class ComprehensiveEvaluator:
    def __init__(self):
        self.evaluation_history = []
        
    def evaluate_predictions(self, predictions: List[Dict], 
                           actuals: List[Dict]) -> Dict[str, float]:
        """Calculate comprehensive set of evaluation metrics"""
        
        metrics = {}
        
        # Basic metrics
        metrics['accuracy'] = self._calculate_accuracy(predictions, actuals)
        metrics['brier_score'] = self._calculate_brier_score(predictions, actuals)
        metrics['log_loss'] = self._calculate_log_loss(predictions, actuals)
        
        # Advanced metrics
        metrics['calibration_error'] = self._calculate_calibration_error(predictions, actuals)
        metrics['discrimination'] = self._calculate_discrimination(predictions, actuals)
        metrics['sharpness'] = self._calculate_sharpness(predictions)
        
        # Outcome-specific metrics
        for outcome in ['home', 'draw', 'away']:
            metrics[f'{outcome}_precision'] = self._calculate_precision(
                predictions, actuals, outcome
            )
            metrics[f'{outcome}_recall'] = self._calculate_recall(
                predictions, actuals, outcome
            )
        
        # Financial metrics
        metrics['roi_flat'] = self._calculate_roi_flat_betting(predictions, actuals)
        metrics['roi_kelly'] = self._calculate_roi_kelly_betting(predictions, actuals)
        
        # Store for trend analysis
        self.evaluation_history.append({
            'timestamp': pd.Timestamp.now(),
            'metrics': metrics,
            'n_predictions': len(predictions)
        })
        
        return metrics
    
    def _calculate_accuracy(self, predictions: List[Dict], actuals: List[Dict]) -> float:
        """Basic accuracy calculation"""
        correct = 0
        for pred, actual in zip(predictions, actuals):
            pred_outcome = np.argmax(pred['probs'])
            actual_outcome = actual['outcome']  # 0=home, 1=draw, 2=away
            if pred_outcome == actual_outcome:
                correct += 1
        
        return correct / len(predictions) if predictions else 0
    
    def _calculate_brier_score(self, predictions: List[Dict], actuals: List[Dict]) -> float:
        """Multi-class Brier score"""
        scores = []
        for pred, actual in zip(predictions, actuals):
            probs = pred['probs']
            outcome_vector = [0, 0, 0]
            outcome_vector[actual['outcome']] = 1
            
            score = sum((probs[i] - outcome_vector[i])**2 for i in range(3))
            scores.append(score)
        
        return np.mean(scores) if scores else 0
    
    def _calculate_log_loss(self, predictions: List[Dict], actuals: List[Dict]) -> float:
        """Logarithmic loss (cross-entropy)"""
        losses = []
        for pred, actual in zip(predictions, actuals):
            # Get probability of actual outcome
            actual_prob = pred['probs'][actual['outcome']]
            # Avoid log(0)
            loss = -np.log(max(actual_prob, 1e-15))
            losses.append(loss)
        
        return np.mean(losses) if losses else 0
    
    def _calculate_calibration_error(self, predictions: List[Dict], 
                                   actuals: List[Dict]) -> float:
        """Expected Calibration Error (ECE)"""
        n_bins = 10
        bin_boundaries = np.linspace(0, 1, n_bins + 1)
        
        total_error = 0
        total_count = 0
        
        for outcome in range(3):  # home, draw, away
            # Extract probabilities and outcomes for this class
            probs = [p['probs'][outcome] for p in predictions]
            outcomes = [1 if a['outcome'] == outcome else 0 for a in actuals]
            
            # Calculate calibration error per bin
            for i in range(n_bins):
                lower, upper = bin_boundaries[i], bin_boundaries[i+1]
                
                # Find predictions in this bin
                in_bin = [(p, o) for p, o in zip(probs, outcomes) 
                         if lower <= p < upper]
                
                if in_bin:
                    bin_probs, bin_outcomes = zip(*in_bin)
                    avg_confidence = np.mean(bin_probs)
                    avg_accuracy = np.mean(bin_outcomes)
                    
                    # Weighted error
                    bin_error = abs(avg_confidence - avg_accuracy) * len(in_bin)
                    total_error += bin_error
                    total_count += len(in_bin)
        
        return total_error / total_count if total_count > 0 else 0
    
    def _calculate_discrimination(self, predictions: List[Dict], 
                                actuals: List[Dict]) -> float:
        """How well model discriminates between outcomes"""
        # Use ROC AUC for each outcome
        auc_scores = []
        
        for outcome in range(3):
            probs = [p['probs'][outcome] for p in predictions]
            labels = [1 if a['outcome'] == outcome else 0 for a in actuals]
            
            # Calculate AUC
            from sklearn.metrics import roc_auc_score
            try:
                auc = roc_auc_score(labels, probs)
                auc_scores.append(auc)
            except ValueError:
                # Handle case where only one class present
                continue
        
        return np.mean(auc_scores) if auc_scores else 0.5
    
    def _calculate_sharpness(self, predictions: List[Dict]) -> float:
        """How confident/sharp predictions are"""
        max_probs = [max(p['probs']) for p in predictions]
        return np.mean(max_probs)
    
    def _calculate_roi_flat_betting(self, predictions: List[Dict], 
                                   actuals: List[Dict]) -> float:
        """ROI if betting Â£1 on highest probability outcome"""
        profit = 0
        bets = 0
        
        for pred, actual in zip(predictions, actuals):
            if 'odds' not in pred:
                continue
                
            # Bet on highest probability
            bet_outcome = np.argmax(pred['probs'])
            bets += 1
            
            if bet_outcome == actual['outcome']:
                # Win - get odds payout minus stake
                profit += pred['odds'][bet_outcome] - 1
            else:
                # Lose stake
                profit -= 1
        
        return (profit / bets * 100) if bets > 0 else 0
    
    def _calculate_roi_kelly_betting(self, predictions: List[Dict], 
                                   actuals: List[Dict]) -> float:
        """ROI using Kelly criterion for stake sizing"""
        bankroll = 100
        initial_bankroll = bankroll
        
        for pred, actual in zip(predictions, actuals):
            if 'odds' not in pred or bankroll <= 0:
                continue
            
            # Find positive EV bets
            for outcome in range(3):
                prob = pred['probs'][outcome]
                odds = pred['odds'][outcome]
                
                # Expected value
                ev = prob * odds - 1
                
                if ev > 0:
                    # Kelly fraction
                    kelly_fraction = (prob * odds - 1) / (odds - 1)
                    # Conservative Kelly (25%)
                    stake = min(bankroll * kelly_fraction * 0.25, bankroll * 0.1)
                    
                    if outcome == actual['outcome']:
                        bankroll += stake * (odds - 1)
                    else:
                        bankroll -= stake
                    
                    break  # Only one bet per match
        
        roi = ((bankroll - initial_bankroll) / initial_bankroll * 100)
        return roi
    
    def create_evaluation_report(self, metrics: Dict[str, float]) -> str:
        """Create formatted evaluation report"""
        report = ["=" * 50]
        report.append("MODEL EVALUATION REPORT")
        report.append("=" * 50)
        
        # Basic metrics
        report.append("\nðŸ“Š BASIC METRICS")
        report.append(f"Accuracy: {metrics['accuracy']:.1%}")
        report.append(f"Brier Score: {metrics['brier_score']:.3f} (lower is better)")
        report.append(f"Log Loss: {metrics['log_loss']:.3f} (lower is better)")
        
        # Calibration
        report.append("\nðŸ“ CALIBRATION")
        report.append(f"Calibration Error: {metrics['calibration_error']:.3f}")
        report.append(f"Discrimination (AUC): {metrics['discrimination']:.3f}")
        report.append(f"Sharpness: {metrics['sharpness']:.3f}")
        
        # Per-outcome metrics
        report.append("\nðŸŽ¯ OUTCOME-SPECIFIC PERFORMANCE")
        for outcome in ['home', 'draw', 'away']:
            precision = metrics.get(f'{outcome}_precision', 0)
            recall = metrics.get(f'{outcome}_recall', 0)
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            
            report.append(f"\n{outcome.upper()}:")
            report.append(f"  Precision: {precision:.1%}")
            report.append(f"  Recall: {recall:.1%}")
            report.append(f"  F1-Score: {f1:.1%}")
        
        # Financial performance
        report.append("\nðŸ’° FINANCIAL METRICS")
        report.append(f"ROI (Flat Betting): {metrics['roi_flat']:+.1f}%")
        report.append(f"ROI (Kelly Betting): {metrics['roi_kelly']:+.1f}%")
        
        return "\n".join(report)
    
    def plot_calibration_curve(self, predictions: List[Dict], 
                             actuals: List[Dict]) -> None:
        """Create calibration plot"""
        import plotly.graph_objects as go
        from plotly.subplots import make_subplots
        
        fig = make_subplots(rows=1, cols=3, 
                           subplot_titles=('Home Win', 'Draw', 'Away Win'))
        
        for i, outcome in enumerate(['home', 'draw', 'away']):
            # Get probabilities and outcomes
            probs = [p['probs'][i] for p in predictions]
            outcomes = [1 if a['outcome'] == i else 0 for a in actuals]
            
            # Calculate calibration curve
            fraction_of_positives, mean_predicted_value = calibration_curve(
                outcomes, probs, n_bins=10, strategy='uniform'
            )
            
            # Add perfect calibration line
            fig.add_trace(
                go.Scatter(x=[0, 1], y=[0, 1], 
                          mode='lines',
                          line=dict(dash='dash', color='gray'),
                          name='Perfect',
                          showlegend=(i==0)),
                row=1, col=i+1
            )
            
            # Add actual calibration
            fig.add_trace(
                go.Scatter(x=mean_predicted_value, 
                          y=fraction_of_positives,
                          mode='lines+markers',
                          name=f'{outcome.capitalize()} Calibration',
                          showlegend=False),
                row=1, col=i+1
            )
        
        fig.update_xaxes(title_text="Mean Predicted Probability", range=[0, 1])
        fig.update_yaxes(title_text="Fraction of Positives", range=[0, 1])
        fig.update_layout(title="Model Calibration Analysis", height=400)
        
        fig.write_html("calibration_plot.html")
        logger.info("Calibration plot saved to calibration_plot.html")
```
**Win**: Complete evaluation suite with financial metrics and visualization

### Step 16: Production Monitoring & Alerting System
```python
# shl/monitoring/production_monitor.py
import streamlit as st
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
from typing import Dict, List
import structlog

logger = structlog.get_logger()

class ProductionMonitor:
    def __init__(self, config: Dict):
        self.config = config
        self.alert_thresholds = {
            'accuracy_drop': 0.1,  # 10% drop
            'calibration_degradation': 0.2,  # 20% worse
            'confidence_drift': 0.15,  # 15% change in average confidence
            'roi_threshold': -5.0  # -5% ROI
        }
        
    def run_monitoring_dashboard(self):
        """Main monitoring dashboard"""
        st.set_page_config(page_title="Odds Model Monitor", layout="wide")
        
        st.title("ðŸŽ¯ Allsvenskan Prediction Model Monitor")
        
        # Check system health
        alerts = self.check_system_health()
        if alerts:
            for alert in alerts:
                st.error(f"âš ï¸ {alert['message']}")
        else:
            st.success("âœ… All systems operational")
        
        # Tabs for different views
        tab1, tab2, tab3, tab4, tab5 = st.tabs([
            "ðŸ“Š Performance", "âš¡ Real-time", "ðŸ“ˆ Trends", 
            "ðŸ” Diagnostics", "âš™ï¸ Configuration"
        ])
        
        with tab1:
            self._render_performance_tab()
        
        with tab2:
            self._render_realtime_tab()
        
        with tab3:
            self._render_trends_tab()
        
        with tab4:
            self._render_diagnostics_tab()
        
        with tab5:
            self._render_configuration_tab()
    
    def _render_performance_tab(self):
        """Performance metrics dashboard"""
        st.header("Model Performance Overview")
        
        # Load recent predictions
        recent_7d = self.load_predictions(days=7)
        recent_30d = self.load_predictions(days=30)
        
        # Key metrics
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            acc_7d = self.calculate_accuracy(recent_7d)
            acc_30d = self.calculate_accuracy(recent_30d)
            delta = acc_7d - acc_30d
            st.metric("Accuracy (7d)", f"{acc_7d:.1%}", 
                     f"{delta:+.1%}", delta_color="normal")
        
        with col2:
            brier_7d = self.calculate_brier(recent_7d)
            brier_30d = self.calculate_brier(recent_30d)
            delta = brier_7d - brier_30d
            st.metric("Brier Score (7d)", f"{brier_7d:.3f}", 
                     f"{delta:+.3f}", delta_color="inverse")
        
        with col3:
            roi_7d = self.calculate_roi(recent_7d)
            roi_30d = self.calculate_roi(recent_30d)
            delta = roi_7d - roi_30d
            st.metric("ROI (7d)", f"{roi_7d:+.1%}", 
                     f"{delta:+.1%}", delta_color="normal")
        
        with col4:
            predictions_today = len(self.load_predictions(days=1))
            st.metric("Predictions Today", predictions_today)
        
        # Performance by confidence level
        st.subheader("Performance by Confidence Level")
        perf_by_conf = self.analyze_by_confidence(recent_30d)
        
        fig = self.create_confidence_performance_chart(perf_by_conf)
        st.plotly_chart(fig, use_container_width=True)
        
        # Outcome distribution
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("Predicted vs Actual Outcomes")
            outcome_comparison = self.compare_outcome_distributions(recent_30d)
            fig = self.create_outcome_comparison_chart(outcome_comparison)
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            st.subheader("Calibration Plot")
            fig = self.create_calibration_plot(recent_30d)
            st.plotly_chart(fig, use_container_width=True)
    
    def _render_realtime_tab(self):
        """Real-time monitoring"""
        st.header("Real-time Model Activity")
        
        # Auto-refresh
        placeholder = st.empty()
        
        while True:
            with placeholder.container():
                # Recent predictions
                st.subheader("Last 10 Predictions")
                recent = self.get_recent_predictions(n=10)
                
                # Format for display
                display_df = pd.DataFrame(recent)
                display_df['Confidence'] = display_df['max_prob'].apply(
                    lambda x: f"{x:.1%}"
                )
                display_df['Result'] = display_df.apply(
                    lambda x: "âœ…" if x['correct'] else "âŒ", axis=1
                )
                
                st.dataframe(
                    display_df[['timestamp', 'match', 'prediction', 
                               'Confidence', 'Result']],
                    use_container_width=True
                )
                
                # Live metrics
                st.subheader("Rolling Metrics (Last 50 predictions)")
                
                rolling = self.calculate_rolling_metrics(window=50)
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Rolling Accuracy", f"{rolling['accuracy']:.1%}")
                with col2:
                    st.metric("Avg Confidence", f"{rolling['avg_confidence']:.1%}")
                with col3:
                    st.metric("High Conf Accuracy", f"{rolling['high_conf_acc']:.1%}")
                
                # Update every 30 seconds
                time.sleep(30)
    
    def check_system_health(self) -> List[Dict]:
        """Check for system issues"""
        alerts = []
        
        # Performance degradation
        recent_perf = self.get_recent_performance_metrics()
        historical_perf = self.get_historical_performance_metrics()
        
        # Accuracy check
        if recent_perf['accuracy'] < historical_perf['accuracy'] * (1 - self.alert_thresholds['accuracy_drop']):
            alerts.append({
                'level': 'warning',
                'message': f"Accuracy dropped from {historical_perf['accuracy']:.1%} to {recent_perf['accuracy']:.1%}",
                'metric': 'accuracy'
            })
        
        # Calibration check
        if recent_perf['calibration_error'] > historical_perf['calibration_error'] * (1 + self.alert_thresholds['calibration_degradation']):
            alerts.append({
                'level': 'warning',
                'message': f"Calibration degraded: ECE increased to {recent_perf['calibration_error']:.3f}",
                'metric': 'calibration'
            })
        
        # ROI check
        if recent_perf['roi'] < self.alert_thresholds['roi_threshold']:
            alerts.append({
                'level': 'error',
                'message': f"ROI below threshold: {recent_perf['roi']:.1%}",
                'metric': 'roi'
            })
        
        # Data freshness
        latest_prediction = self.get_latest_prediction_time()
        if latest_prediction and (datetime.now() - latest_prediction).hours > 24:
            alerts.append({
                'level': 'warning',
                'message': "No predictions in last 24 hours",
                'metric': 'freshness'
            })
        
        return alerts
    
    def create_automated_report(self) -> str:
        """Generate automated performance report"""
        report = []
        report.append("# Allsvenskan Model Performance Report")
        report.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        report.append("")
        
        # Executive Summary
        report.append("## Executive Summary")
        
        perf_7d = self.get_performance_summary(days=7)
        perf_30d = self.get_performance_summary(days=30)
        
        report.append(f"- **7-day Accuracy**: {perf_7d['accuracy']:.1%}")
        report.append(f"- **30-day Accuracy**: {perf_30d['accuracy']:.1%}")
        report.append(f"- **7-day ROI**: {perf_7d['roi']:+.1%}")
        report.append(f"- **Total Predictions (30d)**: {perf_30d['n_predictions']}")
        report.append("")
        
        # Alerts
        alerts = self.check_system_health()
        if alerts:
            report.append("## âš ï¸ Alerts")
            for alert in alerts:
                report.append(f"- {alert['message']}")
            report.append("")
        
        # Detailed Metrics
        report.append("## Detailed Performance Metrics")
        
        # Add performance table
        metrics_df = self.create_metrics_summary_table()
        report.append(metrics_df.to_markdown())
        
        # Recommendations
        report.append("\n## Recommendations")
        recommendations = self.generate_recommendations(perf_7d, perf_30d, alerts)
        for rec in recommendations:
            report.append(f"- {rec}")
        
        return "\n".join(report)
    
    def generate_recommendations(self, perf_7d: Dict, perf_30d: Dict, 
                               alerts: List[Dict]) -> List[str]:
        """Generate actionable recommendations"""
        recommendations = []
        
        # Based on performance trends
        if perf_7d['accuracy'] < perf_30d['accuracy'] * 0.95:
            recommendations.append(
                "Consider retraining model - recent accuracy declining"
            )
        
        if perf_7d['calibration_error'] > 0.1:
            recommendations.append(
                "Calibration poor - review probability regularization parameters"
            )
        
        # Based on alerts
        for alert in alerts:
            if alert['metric'] == 'roi' and alert['level'] == 'error':
                recommendations.append(
                    "Review betting strategy - current approach unprofitable"
                )
        
        # Based on data analysis
        if perf_7d['avg_confidence'] > 0.7:
            recommendations.append(
                "Model showing high confidence - verify against overconfidence"
            )
        
        if not recommendations:
            recommendations.append("Model performing within expected parameters")
        
        return recommendations
```
**Win**: Complete production monitoring with automated alerts and reporting

## Summary: Complete Implementation Timeline

### Week 1: Foundation with Robustness âœ…
- Schema validation, error handling, logging (Steps 1-3)
- API abstraction, caching, configuration
- Unit tests and CI/CD setup

### Week 2: Core Integration âœ…
- Robust hybrid model with validation (Steps 4-5)
- Parallelized Monte Carlo simulation (Step 6)
- Integration testing

### Week 3: Advanced Features âœ…
- Sophisticated strength extraction (Steps 7-8)
- Multi-dimensional volatility analysis (Step 9)
- Adaptive window selection (Step 10)
- Confidence scoring system (Step 11)

### Week 4: Optimization & Polish âœ…
- Grid search with cross-validation (Step 12)
- Bayesian optimization (Step 13)
- Probability regularization (Step 14)
- Comprehensive evaluation (Step 15)

### Week 5: Production & Monitoring âœ…
- Production dashboard (Step 16)
- Automated alerting
- Performance reporting
- Documentation

**Total: 5 weeks for complete production-ready system**

## Key Deliverables:
1. **Robust data pipeline** with validation and caching
2. **Sophisticated hybrid model** with dynamic weighting
3. **Adaptive system** that learns optimal parameters
4. **Comprehensive monitoring** with automated alerts
5. **Complete evaluation framework** with financial metrics
6. **Production-ready code** with testing and documentation## Phase 4: Dynamic Parameters with Testing (Medium-Hard)

### Step 9: Comprehensive Volatility Analysis
```python
# shl/analysis/volatility.py
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple
import structlog

logger = structlog.get_logger()

class VolatilityAnalyzer:
    def __init__(self):
        self.volatility_cache = {}
        
    def calculate_team_volatility(self, team: str, results: pd.DataFrame, 
                                 window: int = 10) -> Dict[str, float]:
        """Calculate multi-dimensional volatility metrics"""
        cache_key = f"{team}_{len(results)}_{window}"
        if cache_key in self.volatility_cache:
            return self.volatility_cache[cache_key]
        
        try:
            # Get team's recent matches
            team_matches = self._get_team_matches(team, results, window)
            
            if len(team_matches) < 5:
                return {'overall': 0.5, 'points': 0.5, 'goals': 0.5, 'form': 0.5}
            
            # Calculate different volatility dimensions
            points_vol = self._calculate_points_volatility(team_matches)
            goals_vol = self._calculate_goals_volatility(team_matches)
            form_vol = self._calculate_form_volatility(team_matches)
            
            # Overall volatility (weighted average)
            overall_vol = (
                0.5 * points_vol + 
                0.3 * goals_vol + 
                0.2 * form_vol
            )
            
            volatility = {
                'overall': round(overall_vol, 3),
                'points': round(points_vol, 3),
                'goals': round(goals_vol, 3),
                'form': round(form_vol, 3)
            }
            
            # Cache result
            self.volatility_cache[cache_key] = volatility
            
            return volatility
            
        except Exception as e:
            logger.error(f"Error calculating volatility for {team}: {e}")
            return {'overall': 0.5, 'points': 0.5, 'goals': 0.5, 'form': 0.5}
    
    def _calculate_points_volatility(self, matches: pd.DataFrame) -> float:
        """Volatility in points earned"""
        points = []
        for _, match in matches.iterrows():
            if match['home_score'] > match['away_score']:
                pts = 3 if match['is_home'] else 0
            elif match['home_score'] < match['away_score']:
                pts = 0 if match['is_home'] else 3
            else:
                pts = 1
            points.append(pts)
        
        if np.mean(points) > 0:
            return min(1.0, np.std(points) / np.mean(points))
        return 0.5
    
    def _calculate_goals_volatility(self, matches: pd.DataFrame) -> float:
        """Volatility in goals scored/conceded"""
        goals_for = matches['team_goals'].values
        goals_against = matches['opp_goals'].values
        
        # Normalized standard deviation
        gf_vol = np.std(goals_for) / (np.mean(goals_for) + 0.1)
        ga_vol = np.std(goals_against) / (np.mean(goals_against) + 0.1)
        
        return min(1.0, (gf_vol + ga_vol) / 2)
    
    def _calculate_form_volatility(self, matches: pd.DataFrame) -> float:
        """Volatility in rolling form"""
        if len(matches) < 5:
            return 0.5
        
        # Calculate 3-game rolling average
        points = matches['points'].values
        rolling_avg = pd.Series(points).rolling(3, min_periods=1).mean()
        
        # Volatility of the rolling average
        return min(1.0, np.std(rolling_avg) / (np.mean(rolling_avg) + 0.1))
    
    def get_stability_category(self, volatility: Dict[str, float]) -> str:
        """Categorize team stability for reporting"""
        overall = volatility['overall']
        if overall < 0.3:
            return "Very Stable"
        elif overall < 0.5:
            return "Stable"
        elif overall < 0.7:
            return "Volatile"
        else:
            return "Very Volatile"

# tests/test_volatility.py
import pytest
from src.analysis.volatility import VolatilityAnalyzer

def test_volatility_calculation():
    analyzer = VolatilityAnalyzer()
    
    # Mock stable team data
    stable_results = pd.DataFrame({
        'team_goals': [1, 1, 2, 1, 1, 2, 1, 1, 1, 2],
        'opp_goals': [0, 1, 1, 0, 1, 1, 0, 1, 0, 1],
        'points': [3, 1, 3, 3, 1, 3, 3, 1, 3, 3]
    })
    
    volatility = analyzer.calculate_team_volatility('StableFC', stable_results)
    assert volatility['overall'] < 0.5
    assert analyzer.get_stability_category(volatility) in ["Stable", "Very Stable"]
    
    # Mock volatile team data
    volatile_results = pd.DataFrame({
        'team_goals': [0, 4, 1, 3, 0, 5, 1, 0, 4, 1],
        'opp_goals': [3, 0, 2, 1, 4, 1, 1, 3, 2, 0],
        'points': [0, 3, 0, 3, 0, 3, 1, 0, 3, 3]
    })
    
    volatility = analyzer.calculate_team_volatility('VolatileFC', volatile_results)
    assert volatility['overall'] > 0.5
    assert analyzer.get_stability_category(volatility) in ["Volatile", "Very Volatile"]
```
**Win**: Multi-dimensional volatility analysis with testing

### Step 10: Adaptive Window Selection
```python
# shl/analysis/adaptive_window.py
from sklearn.model_selection import TimeSeriesSplit
from typing import List, Tuple
import numpy as np
import structlog

logger = structlog.get_logger()

class AdaptiveWindowSelector:
    def __init__(self):
        self.window_performance = {}
        self.optimal_windows = {}
        
    def find_optimal_window(self, team: str, results_df: pd.DataFrame,
                           window_candidates: List[int] = None) -> int:
        """Find optimal lookback window via time series cross-validation"""
        if window_candidates is None:
            window_candidates = [5, 8, 10, 12, 15, 20]
        
        # Filter to team's matches
        team_matches = results_df[
            (results_df['home_team'] == team) | 
            (results_df['away_team'] == team)
        ].copy()
        
        if len(team_matches) < 20:
            logger.info(f"Insufficient data for {team}, using default window")
            return min(10, len(team_matches))
        
        # Time series cross-validation
        tscv = TimeSeriesSplit(n_splits=3)
        window_scores = {}
        
        for window in window_candidates:
            if window >= len(team_matches) - 5:
                continue
                
            scores = []
            
            for train_idx, test_idx in tscv.split(team_matches):
                if len(train_idx) < window:
                    continue
                
                # Calculate performance for this window
                score = self._evaluate_window_performance(
                    team_matches.iloc[train_idx],
                    team_matches.iloc[test_idx],
                    window
                )
                scores.append(score)
            
            if scores:
                window_scores[window] = np.mean(scores)
        
        # Select best window
        if window_scores:
            optimal_window = min(window_scores, key=window_scores.get)
            self.optimal_windows[team] = optimal_window
            logger.info(f"Optimal window for {team}: {optimal_window} games")
            return optimal_window
        else:
            return 10  # Default
    
    def _evaluate_window_performance(self, train_data: pd.DataFrame,
                                   test_data: pd.DataFrame,
                                   window: int) -> float:
        """Evaluate prediction performance with given window"""
        # Use last 'window' games from training to predict test
        if len(train_data) < window:
            return float('inf')
        
        recent_train = train_data.tail(window)
        
        # Calculate simple performance metrics
        train_avg_goals = recent_train['team_goals'].mean()
        test_avg_goals = test_data['team_goals'].mean()
        
        # Mean absolute error
        mae = abs(train_avg_goals - test_avg_goals)
        
        # Penalize very small windows (less stable)
        if window < 8:
            mae *= 1.2
        
        return mae
    
    def get_adaptive_windows_report(self) -> pd.DataFrame:
        """Generate report of optimal windows by team"""
        data = []
        for team, window in self.optimal_windows.items():
            data.append({
                'Team': team,
                'Optimal_Window': window,
                'Last_Updated': pd.Timestamp.now()
            })
        
        return pd.DataFrame(data)

# Integration with main system
class AdaptiveStrengthCalculator:
    def __init__(self):
        self.window_selector = AdaptiveWindowSelector()
        self.strength_calculator = OddsStrengthCalculator()
        
    def calculate_adaptive_strength(self, team: str, results_df: pd.DataFrame,
                                  odds_history: List[Dict]) -> Dict[str, float]:
        """Calculate strength with adaptive window"""
        # Find optimal window
        optimal_window = self.window_selector.find_optimal_window(team, results_df)
        
        # Calculate strength with optimal window
        strength = self.strength_calculator.calculate_team_strength(
            team, odds_history, lookback=optimal_window
        )
        
        # Add window info to output
        strength['window_used'] = optimal_window
        
        return strength
```
**Win**: Data-driven window selection for each team

### Step 11: Multi-Source Confidence System
```python
# shl/models/confidence_system.py
import numpy as np
from typing import List, Dict, Optional
import structlog

logger = structlog.get_logger()

class OddsConfidenceCalculator:
    def __init__(self, min_bookmakers: int = 3):
        self.min_bookmakers = min_bookmakers
        self.confidence_history = []
        
    def calculate_comprehensive_confidence(self, 
                                         bookmaker_odds: List[Dict],
                                         historical_context: Optional[Dict] = None) -> Dict[str, float]:
        """Calculate multi-factor confidence score"""
        if len(bookmaker_odds) < self.min_bookmakers:
            logger.warning(f"Only {len(bookmaker_odds)} bookmakers, low confidence")
            return {
                'overall': 0.3,
                'agreement': 0.0,
                'liquidity': 0.3,
                'stability': 0.5
            }
        
        # Calculate different confidence dimensions
        agreement_conf = self._calculate_agreement_confidence(bookmaker_odds)
        liquidity_conf = self._calculate_liquidity_confidence(bookmaker_odds)
        stability_conf = self._calculate_stability_confidence(bookmaker_odds)
        
        # Historical alignment (if available)
        historical_conf = 0.5
        if historical_context:
            historical_conf = self._calculate_historical_alignment(
                bookmaker_odds, historical_context
            )
        
        # Weighted overall confidence
        overall_conf = (
            0.4 * agreement_conf +
            0.3 * liquidity_conf +
            0.2 * stability_conf +
            0.1 * historical_conf
        )
        
        confidence = {
            'overall': round(overall_conf, 3),
            'agreement': round(agreement_conf, 3),
            'liquidity': round(liquidity_conf, 3),
            'stability': round(stability_conf, 3),
            'historical': round(historical_conf, 3)
        }
        
        # Store for analysis
        self.confidence_history.append({
            'timestamp': pd.Timestamp.now(),
            'confidence': confidence,
            'n_bookmakers': len(bookmaker_odds)
        })
        
        return confidence
    
    def _calculate_agreement_confidence(self, bookmaker_odds: List[Dict]) -> float:
        """Confidence based on bookmaker agreement"""
        # Extract probabilities for each outcome
        home_probs = []
        draw_probs = []
        away_probs = []
        
        for odds in bookmaker_odds:
            try:
                h, d, a = remove_margin(
                    odds['home'], odds['draw'], odds['away']
                )
                home_probs.append(h)
                draw_probs.append(d)
                away_probs.append(a)
            except Exception as e:
                logger.warning(f"Invalid odds format: {e}")
                continue
        
        if len(home_probs) < 2:
            return 0.0
        
        # Calculate coefficient of variation for each outcome
        cv_home = np.std(home_probs) / (np.mean(home_probs) + 0.01)
        cv_draw = np.std(draw_probs) / (np.mean(draw_probs) + 0.01)
        cv_away = np.std(away_probs) / (np.mean(away_probs) + 0.01)
        
        # Average CV (lower is better)
        avg_cv = (cv_home + cv_draw + cv_away) / 3
        
        # Convert to confidence (0-1 scale)
        confidence = 1 / (1 + avg_cv * 5)
        
        return confidence
    
    def _calculate_liquidity_confidence(self, bookmaker_odds: List[Dict]) -> float:
        """Confidence based on market liquidity indicators"""
        # More bookmakers = more liquidity
        n_books = len(bookmaker_odds)
        
        # Check for major bookmakers (would need bookmaker names in real impl)
        major_books = ['bet365', 'pinnacle', 'betfair']
        major_count = sum(1 for odds in bookmaker_odds 
                         if odds.get('bookmaker', '').lower() in major_books)
        
        # Liquidity score
        base_score = min(1.0, n_books / 10)  # Max out at 10 bookmakers
        major_bonus = major_count * 0.1
        
        return min(1.0, base_score + major_bonus)
    
    def _calculate_stability_confidence(self, bookmaker_odds: List[Dict]) -> float:
        """Confidence based on odds stability over time"""
        # In real implementation, would track odds movements
        # For now, check if odds are within reasonable ranges
        
        all_odds = []
        for odds in bookmaker_odds:
            all_odds.extend([odds['home'], odds['draw'], odds['away']])
        
        # Check for suspicious odds
        suspicious_count = sum(1 for odd in all_odds if odd < 1.1 or odd > 50)
        
        if suspicious_count > 0:
            return 0.5 - (suspicious_count * 0.1)
        
        # Check spread between min and max
        home_odds = [odds['home'] for odds in bookmaker_odds]
        if home_odds:
            spread = (max(home_odds) - min(home_odds)) / min(home_odds)
            if spread > 0.2:  # More than 20% spread
                return 0.7
            elif spread > 0.1:
                return 0.85
            else:
                return 1.0
        
        return 0.8
    
    def _calculate_historical_alignment(self, bookmaker_odds: List[Dict],
                                       historical_context: Dict[str, float]) -> float:
        """Check if odds align with historical performance"""
        # Average the bookmaker odds
        avg_odds = self._average_bookmaker_odds(bookmaker_odds)
        implied_probs = remove_margin(
            avg_odds['home'], avg_odds['draw'], avg_odds['away']
        )
        
        # Compare with historical win rates
        historical_home_rate = historical_context.get('home_win_rate', 0.45)
        historical_draw_rate = historical_context.get('draw_rate', 0.25)
        
        # Calculate alignment
        home_diff = abs(implied_probs[0] - historical_home_rate)
        draw_diff = abs(implied_probs[1] - historical_draw_rate)
        
        # Convert to confidence
        avg_diff = (home_diff + draw_diff) / 2
        confidence = 1 / (1 + avg_diff * 5)
        
        return confidence
    
    def _average_bookmaker_odds(self, bookmaker_odds: List[Dict]) -> Dict[str, float]:
        """Calculate average odds across bookmakers"""
        home_odds = [odds['home'] for odds in bookmaker_odds]
        draw_odds = [odds['draw'] for odds in bookmaker_odds]
        away_odds = [odds['away'] for odds in bookmaker_odds]
        
        return {
            'home': np.mean(home_odds),
            'draw': np.mean(draw_odds),
            'away': np.mean(away_odds)
        }

# Integration with weight adjustment
def adjust_weight_by_confidence(base_weight: float, confidence: Dict[str, float]) -> float:
    """Adjust odds weight based on confidence metrics"""
    overall_confidence = confidence['overall']
    
    # Scale adjustment based on confidence
    if overall_confidence < 0.4:
        # Low confidence - reduce weight significantly
        adjustment = 0.5
    elif overall_confidence < 0.6:
        # Medium confidence - moderate reduction
        adjustment = 0.8
    elif overall_confidence < 0.8:
        # Good confidence - slight reduction
        adjustment = 0.95
    else:
        # High confidence - use full weight
        adjustment = 1.0
    
    adjusted_weight = base_weight * adjustment
    
    logger.info(f"Adjusted weight from {base_weight:.3f} to {adjusted_weight:.3f} "
                f"based on confidence {overall_confidence:.3f}")
    
    return adjusted_weight
```
**Win**: Sophisticated confidence scoring with multiple factors# Step-by-Step Implementation Plan: Odds Integration for Allsvenskan Prediction

## Phase 1: Foundation & Quick Wins (Easy)

### Step 1: Data Schema & Validation
```python
# shl/data/odds_schema.py
from pydantic import BaseModel, validator
from datetime import datetime

class OddsRecord(BaseModel):
    date: datetime
    home_team: str
    away_team: str
    home_odds: float
    draw_odds: float
    away_odds: float
    bookmaker: str = "aggregate"
    
    @validator('home_odds', 'draw_odds', 'away_odds')
    def odds_must_be_valid(cls, v):
        if v < 1.01 or v > 100:
            raise ValueError(f'Odds must be between 1.01 and 100, got {v}')
        return v

class OddsData:
    def __init__(self):
        self.odds_cache = {}  # {match_key: OddsRecord}
        self.logger = setup_logger(__name__)
    
    def add_match_odds(self, odds_record: OddsRecord):
        """Store validated odds with error handling"""
        try:
            key = f"{odds_record.date}_{odds_record.home_team}_{odds_record.away_team}"
            self.odds_cache[key] = odds_record
            self.logger.info(f"Added odds for {key}")
        except Exception as e:
            self.logger.error(f"Failed to add odds: {e}")
            raise
```
**Win**: Type-safe data storage with validation

### Step 2: Robust Odds Converter with Logging
```python
# shl/utils/odds_converter.py
import logging
from typing import Tuple, Optional

logger = logging.getLogger(__name__)

def odds_to_probability(decimal_odds: float) -> float:
    """Convert single odds to probability with validation"""
    if decimal_odds < 1.01:
        logger.warning(f"Odds {decimal_odds} below minimum, setting to 1.01")
        decimal_odds = 1.01
    return 1.0 / decimal_odds

def remove_margin(home_odds: float, draw_odds: float, away_odds: float) -> Tuple[float, float, float]:
    """Remove bookmaker margin with error handling"""
    try:
        total = 1/home_odds + 1/draw_odds + 1/away_odds
        if total < 1.0:
            logger.warning(f"Negative margin detected: {total}, possible arbitrage")
        
        probs = (1/home_odds/total, 1/draw_odds/total, 1/away_odds/total)
        
        # Sanity check
        if not (0.99 < sum(probs) < 1.01):
            logger.error(f"Probability sum {sum(probs)} not close to 1.0")
        
        return probs
    except ZeroDivisionError:
        logger.error("Zero odds detected")
        return (0.33, 0.33, 0.34)  # Fallback to uniform
```
**Win**: Bulletproof odds conversion with comprehensive logging

### Step 3: Configuration Management & Testing
```python
# shl/config/odds_config.yaml
odds_integration:
  weights:
    games_1_2: 0.7
    games_3_5: 0.5
    games_6_10: 0.3
    games_11_plus: 0.1
  validation:
    min_odds: 1.01
    max_odds: 100.0
    min_bookmakers: 3
  cache:
    ttl_seconds: 3600
    max_size: 10000

# shl/config/config_loader.py
import yaml
from functools import lru_cache

@lru_cache(maxsize=1)
def load_config():
    """Load configuration with caching"""
    with open('shl/config/odds_config.yaml', 'r') as f:
        return yaml.safe_load(f)

# tests/test_odds_converter.py
import pytest
from src.utils.odds_converter import odds_to_probability, remove_margin

def test_odds_to_probability():
    assert abs(odds_to_probability(2.0) - 0.5) < 0.001
    assert abs(odds_to_probability(1.01) - 0.99) < 0.01

def test_remove_margin():
    probs = remove_margin(2.0, 3.5, 4.0)
    assert abs(sum(probs) - 1.0) < 0.001
    assert all(0 < p < 1 for p in probs)

def test_arbitrage_detection():
    # Arbitrage situation (total < 1.0)
    probs = remove_margin(2.2, 3.8, 4.5)
    assert sum(probs) == pytest.approx(1.0)
```
**Win**: Configuration externalized, unit tests in place

## Phase 2: Basic Integration with Error Handling (Easy-Medium)

### Step 4: API Abstraction Layer
```python
# shl/data/odds_source.py
from abc import ABC, abstractmethod
from typing import Dict, List
import redis
from datetime import datetime, timedelta

class OddsSource(ABC):
    @abstractmethod
    def get_match_odds(self, home_team: str, away_team: str, date: datetime) -> Dict:
        pass

class CSVOddsSource(OddsSource):
    def __init__(self, csv_path: str):
        self.odds_df = pd.read_csv(csv_path)
        self.logger = logging.getLogger(__name__)
        
    def get_match_odds(self, home_team: str, away_team: str, date: datetime) -> Dict:
        try:
            match = self.odds_df[
                (self.odds_df['home_team'] == home_team) &
                (self.odds_df['away_team'] == away_team) &
                (self.odds_df['date'] == date.strftime('%Y-%m-%d'))
            ]
            if match.empty:
                return None
            return match.iloc[0].to_dict()
        except Exception as e:
            self.logger.error(f"Error fetching odds: {e}")
            return None

class APIOpdsSource(OddsSource):
    def __init__(self, api_key: str, cache_client: redis.Redis):
        self.api_key = api_key
        self.cache = cache_client
        self.cache_ttl = 3600  # 1 hour
        
    def get_match_odds(self, home_team: str, away_team: str, date: datetime) -> Dict:
        # Check cache first
        cache_key = f"odds:{home_team}:{away_team}:{date}"
        cached = self.cache.get(cache_key)
        if cached:
            return json.loads(cached)
        
        # Fetch from API
        odds = self._fetch_from_api(home_team, away_team, date)
        if odds:
            self.cache.setex(cache_key, self.cache_ttl, json.dumps(odds))
        return odds
```
**Win**: Flexible data source switching with caching

### Step 5: Robust Hybrid Model with Validation
```python
# shl/models/hybrid_model.py
import structlog
from src.config.config_loader import load_config

logger = structlog.get_logger()

class HybridModel:
    def __init__(self, poisson_model):
        self.poisson_model = poisson_model
        self.config = load_config()
        self.weights = self.config['odds_integration']['weights']
        
    def predict_with_odds(self, home_team: str, away_team: str, 
                         odds_data: Dict, games_until_match: int = 1) -> Tuple[float, float, float]:
        """Blend Poisson and odds predictions with validation"""
        try:
            # Validate inputs
            if not odds_data or any(k not in odds_data for k in ['home', 'draw', 'away']):
                logger.warning(f"Invalid odds data for {home_team} vs {away_team}, using Poisson only")
                return self.poisson_model.predict_probabilities(home_team, away_team)
            
            # Get Poisson prediction
            pois_probs = self.poisson_model.predict_probabilities(home_team, away_team)
            
            # Get odds prediction with validation
            try:
                odds_probs = remove_margin(
                    float(odds_data['home']), 
                    float(odds_data['draw']), 
                    float(odds_data['away'])
                )
            except (ValueError, TypeError) as e:
                logger.error(f"Invalid odds values: {e}")
                return pois_probs
            
            # Dynamic weight based on games until match
            weight = self._get_dynamic_weight(games_until_match)
            
            # Blend predictions
            final_probs = tuple(
                weight * odds_p + (1 - weight) * pois_p 
                for odds_p, pois_p in zip(odds_probs, pois_probs)
            )
            
            # Validate output
            if not (0.99 < sum(final_probs) < 1.01):
                logger.error(f"Invalid probability sum: {sum(final_probs)}")
                # Renormalize
                total = sum(final_probs)
                final_probs = tuple(p / total for p in final_probs)
            
            logger.info(f"Predicted {home_team} vs {away_team}: {final_probs}")
            return final_probs
            
        except Exception as e:
            logger.exception(f"Error in hybrid prediction: {e}")
            return self.poisson_model.predict_probabilities(home_team, away_team)
    
    def _get_dynamic_weight(self, games_until_match: int) -> float:
        """Get weight from config based on games until match"""
        if games_until_match <= 2:
            return self.weights['games_1_2']
        elif games_until_match <= 5:
            return self.weights['games_3_5']
        elif games_until_match <= 10:
            return self.weights['games_6_10']
        else:
            return self.weights['games_11_plus']
```
**Win**: Production-ready hybrid model with error handling

### Step 6: Parallelized Monte Carlo Simulation
```python
# shl/simulation/parallel_simulator.py
from joblib import Parallel, delayed
import numpy as np
from typing import List, Dict
import structlog

logger = structlog.get_logger()

class ParallelOddsSimulator:
    def __init__(self, hybrid_model, n_jobs=-1):
        self.hybrid_model = hybrid_model
        self.n_jobs = n_jobs
        
    def simulate_season(self, fixtures_df, odds_dict, n_sims=10000):
        """Parallel Monte Carlo simulation with odds integration"""
        # Split simulations across cores
        n_sims_per_job = n_sims // self.n_jobs
        
        results = Parallel(n_jobs=self.n_jobs)(
            delayed(self._simulate_batch)(
                fixtures_df, odds_dict, n_sims_per_job, seed=i
            ) for i in range(self.n_jobs)
        )
        
        # Flatten results
        return [sim for batch in results for sim in batch]
    
    def _simulate_batch(self, fixtures_df, odds_dict, n_sims, seed):
        """Simulate a batch of seasons"""
        np.random.seed(seed)
        batch_results = []
        
        # Identify phases
        next_round = fixtures_df.iloc[:16]  # 2 rounds
        remaining = fixtures_df.iloc[16:]
        
        for sim in range(n_sims):
            try:
                sim_results = {}
                
                # Phase 1: Use odds for next 2 rounds
                for idx, match in next_round.iterrows():
                    match_key = f"{match['date']}_{match['home_team']}_{match['away_team']}"
                    
                    if match_key in odds_dict:
                        probs = self.hybrid_model.predict_with_odds(
                            match['home_team'], 
                            match['away_team'],
                            odds_dict[match_key],
                            games_until_match=1
                        )
                    else:
                        logger.warning(f"No odds for {match_key}, using Poisson")
                        probs = self.hybrid_model.poisson_model.predict_probabilities(
                            match['home_team'], match['away_team']
                        )
                    
                    outcome = self._simulate_match_outcome(probs)
                    sim_results[match_key] = outcome
                
                # Phase 2: Remaining season with decaying odds influence
                for idx, match in remaining.iterrows():
                    games_until = idx + 1  # Simplified calculation
                    
                    # Use blended model even without direct odds
                    probs = self.hybrid_model.predict_with_odds(
                        match['home_team'],
                        match['away_team'], 
                        odds_dict.get(f"historical_{match['home_team']}_{match['away_team']}"),
                        games_until_match=games_until
                    )
                    
                    outcome = self._simulate_match_outcome(probs)
                    sim_results[f"{match['date']}_{match['home_team']}_{match['away_team']}"] = outcome
                
                batch_results.append(sim_results)
                
            except Exception as e:
                logger.error(f"Error in simulation {sim}: {e}")
                continue
        
        return batch_results
    
    def _simulate_match_outcome(self, probs):
        """Simulate match based on probabilities"""
        outcome = np.random.choice(['home', 'draw', 'away'], p=probs)
        return outcome
```
**Win**: Fast parallel simulations with comprehensive error handling

## Phase 3: Strength Extraction with Caching (Medium)

### Step 7: Cached Odds Strength Calculator
```python
# shl/analysis/odds_strength.py
from functools import lru_cache
from typing import Dict, Tuple, List
import numpy as np
import structlog

logger = structlog.get_logger()

class OddsStrengthCalculator:
    def __init__(self, cache_size=128):
        self.cache_size = cache_size
        self._strength_cache = {}
        
    def calculate_team_strength(self, team: str, odds_history: List[Dict], 
                               lookback: int = None) -> Dict[str, float]:
        """Extract attack/defense ratings from odds with caching"""
        # Create cache key
        cache_key = f"{team}_{len(odds_history)}_{lookback}"
        
        if cache_key in self._strength_cache:
            logger.debug(f"Using cached strength for {team}")
            return self._strength_cache[cache_key]
        
        try:
            # Determine optimal lookback if not specified
            if lookback is None:
                lookback = self._find_optimal_lookback(team, odds_history)
            
            # Get recent games
            recent_games = odds_history[-lookback:]
            
            # Calculate strengths
            strengths = self._calculate_strengths_from_odds(team, recent_games)
            
            # Cache result
            self._strength_cache[cache_key] = strengths
            
            # Manage cache size
            if len(self._strength_cache) > self.cache_size:
                # Remove oldest entries
                oldest_key = next(iter(self._strength_cache))
                del self._strength_cache[oldest_key]
            
            return strengths
            
        except Exception as e:
            logger.error(f"Error calculating strength for {team}: {e}")
            return {'attack': 1.0, 'defense': 1.0, 'form': 0.5}
    
    def _calculate_strengths_from_odds(self, team: str, games: List[Dict]) -> Dict[str, float]:
        """Core strength calculation with Over/Under integration"""
        if not games:
            return {'attack': 1.0, 'defense': 1.0, 'form': 0.5}
        
        attack_scores = []
        defense_scores = []
        weights = []
        
        for i, game in enumerate(games):
            try:
                # Extract probabilities
                if game['is_home']:
                    win_prob = 1 / game['home_odds']
                    lose_prob = 1 / game['away_odds']
                    
                    # Use Over/Under if available
                    if 'over_2.5_odds' in game and 'under_2.5_odds' in game:
                        over_prob = 1 / game['over_2.5_odds']
                        goals_expectancy = 2.5 + (over_prob - 0.5) * 2
                    else:
                        goals_expectancy = 2.5  # Default
                else:
                    win_prob = 1 / game['away_odds']
                    lose_prob = 1 / game['home_odds']
                    
                    if 'over_2.5_odds' in game:
                        over_prob = 1 / game['over_2.5_odds']
                        goals_expectancy = 2.5 + (over_prob - 0.5) * 2
                    else:
                        goals_expectancy = 2.5
                
                # Normalize probabilities
                draw_prob = 1 / game['draw_odds']
                total_prob = win_prob + draw_prob + lose_prob
                win_prob /= total_prob
                lose_prob /= total_prob
                
                # Calculate strength components
                dominance = win_prob / (win_prob + lose_prob)
                
                # Attack strength correlates with win probability and goal expectancy
                attack_score = dominance * (goals_expectancy / 2.5)
                
                # Defense strength inversely correlates with opponent's implied strength
                defense_score = (1 - lose_prob) * (2.5 / goals_expectancy)
                
                attack_scores.append(attack_score)
                defense_scores.append(defense_score)
                
                # Exponential weighting for recency
                weight = 0.9 ** (len(games) - i - 1)
                weights.append(weight)
                
            except (KeyError, ZeroDivisionError, TypeError) as e:
                logger.warning(f"Skipping invalid game data: {e}")
                continue
        
        if not attack_scores:
            return {'attack': 1.0, 'defense': 1.0, 'form': 0.5}
        
        # Calculate weighted averages
        attack_strength = np.average(attack_scores, weights=weights)
        defense_strength = np.average(defense_scores, weights=weights)
        
        # Calculate form (recent 5 games trend)
        form = self._calculate_form_trend(attack_scores[-5:], weights[-5:])
        
        # Normalize to 0.5-2.0 range
        attack_strength = max(0.5, min(2.0, attack_strength * 1.5))
        defense_strength = max(0.5, min(2.0, defense_strength * 1.5))
        
        return {
            'attack': round(attack_strength, 3),
            'defense': round(defense_strength, 3),
            'form': round(form, 3)
        }
    
    def _calculate_form_trend(self, recent_scores: List[float], weights: List[float]) -> float:
        """Calculate form as trend in recent performance"""
        if len(recent_scores) < 2:
            return 0.5
        
        # Linear regression on recent scores
        x = np.arange(len(recent_scores))
        y = np.array(recent_scores)
        w = np.array(weights) if weights else np.ones_like(y)
        
        # Weighted linear regression
        coeffs = np.polyfit(x, y, 1, w=w)
        slope = coeffs[0]
        
        # Convert slope to 0-1 form rating
        form = 0.5 + np.tanh(slope * 5)  # Sigmoid transformation
        return min(1.0, max(0.0, form))
    
    @lru_cache(maxsize=32)
    def _find_optimal_lookback(self, team: str, max_games: int = 20) -> int:
        """Find optimal lookback window via cross-validation"""
        # This is a simplified version - in production, you'd do proper CV
        # For now, return adaptive window based on number of available games
        if max_games < 10:
            return max_games
        elif max_games < 15:
            return 10
        else:
            return 15
```
**Win**: Sophisticated strength extraction with caching and form analysis

### Step 8: Intelligent Strength Blender
```python
# shl/models/strength_blender.py
import numpy as np
from typing import Dict, Optional
import structlog

logger = structlog.get_logger()

class StrengthBlender:
    def __init__(self, config: Dict):
        self.config = config
        self.blend_history = []  # For analysis
        
    def blend_strengths(self, team: str, 
                       historical_strength: Dict[str, float],
                       odds_strength: Optional[Dict[str, float]],
                       context: Dict) -> Dict[str, float]:
        """Intelligently blend historical and odds-based strengths"""
        
        # If no odds data, return historical
        if not odds_strength:
            logger.info(f"No odds data for {team}, using historical only")
            return historical_strength
        
        # Calculate dynamic weight
        alpha = self._calculate_dynamic_alpha(context)
        
        # Apply confidence adjustment if available
        if 'odds_confidence' in context:
            alpha *= context['odds_confidence']
            logger.debug(f"Adjusted alpha by confidence: {alpha:.3f}")
        
        # Blend each component
        blended = {}
        for component in ['attack', 'defense']:
            if component in historical_strength and component in odds_strength:
                blended[component] = (
                    alpha * odds_strength[component] + 
                    (1 - alpha) * historical_strength[component]
                )
            else:
                blended[component] = historical_strength.get(component, 1.0)
        
        # Handle form specially - more weight on odds-based form
        if 'form' in odds_strength:
            form_alpha = min(alpha * 1.2, 0.8)  # Give more weight to recent form
            blended['form'] = (
                form_alpha * odds_strength['form'] + 
                (1 - form_alpha) * historical_strength.get('form', 0.5)
            )
        
        # Apply bounds and smoothing
        blended = self._apply_bounds_and_smoothing(blended, team)
        
        # Log for analysis
        self._log_blend_decision(team, historical_strength, odds_strength, 
                                blended, alpha, context)
        
        return blended
    
    def _calculate_dynamic_alpha(self, context: Dict) -> float:
        """Calculate weight based on multiple factors"""
        games_until = context.get('games_until_match', 1)
        team_volatility = context.get('team_volatility', 0.5)
        season_progress = context.get('season_progress', 0.5)
        
        # Base weight from config
        if games_until <= 2:
            base_alpha = self.config['odds_integration']['weights']['games_1_2']
        elif games_until <= 5:
            base_alpha = self.config['odds_integration']['weights']['games_3_5']
        elif games_until <= 10:
            base_alpha = self.config['odds_integration']['weights']['games_6_10']
        else:
            base_alpha = self.config['odds_integration']['weights']['games_11_plus']
        
        # Adjust for volatility (volatile teams = trust odds more)
        volatility_adjustment = 1 + (team_volatility - 0.5) * 0.3
        
        # Adjust for season progress (early season = trust historical less)
        if season_progress < 0.2:  # Early season
            season_adjustment = 1.2
        elif season_progress > 0.8:  # Late season
            season_adjustment = 0.9  # Form more stable
        else:
            season_adjustment = 1.0
        
        # Calculate final alpha
        alpha = base_alpha * volatility_adjustment * season_adjustment
        
        # Ensure bounds
        return max(0.1, min(0.9, alpha))
    
    def _apply_bounds_and_smoothing(self, strengths: Dict[str, float], 
                                   team: str) -> Dict[str, float]:
        """Apply reasonable bounds and smooth extreme values"""
        bounded = {}
        
        for key, value in strengths.items():
            if key in ['attack', 'defense']:
                # Enforce bounds
                bounded_value = max(0.5, min(2.0, value))
                
                # Smooth extreme values
                if bounded_value > 1.7:
                    bounded_value = 1.7 + (bounded_value - 1.7) * 0.5
                elif bounded_value < 0.6:
                    bounded_value = 0.6 - (0.6 - bounded_value) * 0.5
                
                bounded[key] = round(bounded_value, 3)
            else:
                bounded[key] = round(value, 3)
        
        return bounded
    
    def _log_blend_decision(self, team: str, historical: Dict, odds: Dict,
                           blended: Dict, alpha: float, context: Dict):
        """Log blending decision for analysis"""
        decision = {
            'team': team,
            'timestamp': pd.Timestamp.now(),
            'alpha': alpha,
            'historical_attack': historical.get('attack'),
            'odds_attack': odds.get('attack'),
            'blended_attack': blended.get('attack'),
            'context': context
        }
        self.blend_history.append(decision)
        
        # Periodic analysis
        if len(self.blend_history) % 100 == 0:
            self._analyze_blend_effectiveness()
    
    def _analyze_blend_effectiveness(self):
        """Analyze how well blending is working"""
        if len(self.blend_history) < 100:
            return
        
        recent = self.blend_history[-100:]
        avg_alpha = np.mean([d['alpha'] for d in recent])
        
        logger.info(f"Blend analysis - Avg alpha: {avg_alpha:.3f}")
```
**Win**: Smart blending with context awareness and self-analysis

## Phase 4: Dynamic Parameters (Medium-Hard)

### Step 9: Team Volatility Calculator
```python
# shl/analysis/volatility.py
def calculate_team_volatility(team_results, window=10):
    """Calculate how volatile a team's performance is"""
    if len(team_results) < window:
        return 0.5  # Default medium volatility
    
    recent = team_results[-window:]
    points = [3 if r['points'] == 3 else (1 if r['points'] == 1 else 0) 
              for r in recent]
    
    if np.mean(points) > 0:
        volatility = np.std(points) / np.mean(points)
    else:
        volatility = 1.0
    
    return min(1.0, volatility)  # Cap at 1.0
```
**Win**: Can now identify volatile vs stable teams

### Step 10: Dynamic Lookback Window
```python
# shl/analysis/dynamic_window.py
def find_optimal_lookback(team, results_df):
    """Find best historical window for each team"""
    windows = [5, 10, 15, 20]
    errors = []
    
    for window in windows:
        # Use first 70% for training, last 30% for validation
        train_size = int(len(results_df) * 0.7)
        train = results_df[:train_size]
        test = results_df[train_size:]
        
        # Calculate strength using this window
        strength = calculate_strength_with_window(team, train, window)
        
        # Predict test matches
        predictions = predict_matches(team, test, strength)
        error = calculate_prediction_error(predictions, test)
        errors.append(error)
    
    # Return window with lowest error
    best_window = windows[np.argmin(errors)]
    return best_window
```
**Win**: Each team gets personalized lookback period

### Step 11: Confidence-Based Weighting
```python
# shl/models/confidence_weight.py
def calculate_odds_confidence(bookmaker_odds_list):
    """Calculate confidence based on bookmaker agreement"""
    if len(bookmaker_odds_list) < 3:
        return 0.5  # Low confidence with few bookmakers
    
    # Convert to probabilities
    probs = [1/odds for odds in bookmaker_odds_list]
    
    # Low std dev = high agreement = high confidence
    std_dev = np.std(probs)
    confidence = 1 / (1 + std_dev * 10)
    
    return confidence

def adjust_weight_by_confidence(base_weight, odds_sources):
    """Adjust odds weight based on bookmaker agreement"""
    confidence = calculate_odds_confidence(odds_sources)
    return base_weight * confidence
```
**Win**: Automatically reduce odds influence when bookmakers disagree

## Phase 5: Automated Calibration (Hard)

### Step 12: Grid Search with Cross-Validation
```python
# shl/optimization/grid_search.py
import itertools
from typing import Dict, List, Tuple
import numpy as np
import pandas as pd
from sklearn.model_selection import TimeSeriesSplit
import structlog

logger = structlog.get_logger()

class WeightOptimizer:
    def __init__(self, config: Dict):
        self.config = config
        self.search_results = []
        
    def optimize_weights_grid_search(self, 
                                   historical_data: pd.DataFrame,
                                   odds_data: Dict,
                                   n_splits: int = 5) -> Dict[str, float]:
        """Grid search with time series cross-validation"""
        
        # Define search space
        param_grid = {
            'games_1_2': [0.6, 0.65, 0.7, 0.75, 0.8],
            'games_3_5': [0.4, 0.45, 0.5, 0.55, 0.6],
            'games_6_10': [0.2, 0.25, 0.3, 0.35, 0.4],
            'games_11_plus': [0.05, 0.1, 0.15, 0.2]
        }
        
        # Ensure decreasing weights
        valid_combinations = []
        for combo in itertools.product(*param_grid.values()):
            if combo[0] > combo[1] > combo[2] > combo[3]:
                valid_combinations.append(dict(zip(param_grid.keys(), combo)))
        
        logger.info(f"Testing {len(valid_combinations)} weight combinations")
        
        # Time series cross-validation
        tscv = TimeSeriesSplit(n_splits=n_splits)
        best_score = float('inf')
        best_weights = None
        
        for weights in valid_combinations:
            scores = []
            
            for train_idx, val_idx in tscv.split(historical_data):
                train_data = historical_data.iloc[train_idx]
                val_data = historical_data.iloc[val_idx]
                
                # Evaluate this weight combination
                score = self._evaluate_weights(
                    weights, train_data, val_data, odds_data
                )
                scores.append(score)
            
            avg_score = np.mean(scores)
            std_score = np.std(scores)
            
            # Store result
            self.search_results.append({
                'weights': weights,
                'avg_score': avg_score,
                'std_score': std_score
            })
            
            # Update best
            if avg_score < best_score:
                best_score = avg_score
                best_weights = weights
                logger.info(f"New best score: {avg_score:.4f} with weights {weights}")
        
        # Analyze results
        self._analyze_search_results()
        
        return best_weights
    
    def _evaluate_weights(self, weights: Dict[str, float],
                         train_data: pd.DataFrame,
                         val_data: pd.DataFrame,
                         odds_data: Dict) -> float:
        """Evaluate weight combination on validation data"""
        # Create temporary model with these weights
        temp_config = self.config.copy()
        temp_config['odds_integration']['weights'] = weights
        
        hybrid_model = HybridModel(PoissonModel())
        hybrid_model.weights = weights
        
        predictions = []
        actuals = []
        
        for _, match in val_data.iterrows():
            match_key = f"{match['date']}_{match['home_team']}_{match['away_team']}"
            
            # Predict with hybrid model
            if match_key in odds_data:
                probs = hybrid_model.predict_with_odds(
                    match['home_team'],
                    match['away_team'],
                    odds_data[match_key],
                    games_until_match=1  # Simplified
                )
            else:
                probs = hybrid_model.poisson_model.predict_probabilities(
                    match['home_team'], match['away_team']
                )
            
            predictions.append(probs)
            
            # Actual result
            if match['home_goals'] > match['away_goals']:
                actual = [1, 0, 0]
            elif match['home_goals'] < match['away_goals']:
                actual = [0, 0, 1]
            else:
                actual = [0, 1, 0]
            actuals.append(actual)
        
        # Calculate Brier score
        brier_score = self._calculate_brier_score(predictions, actuals)
        
        return brier_score
    
    def _calculate_brier_score(self, predictions: List[Tuple], 
                              actuals: List[List]) -> float:
        """Calculate average Brier score"""
        scores = []
        for pred, actual in zip(predictions, actuals):
            score = sum((pred[i] - actual[i])**2 for i in range(3))
            scores.append(score)
        return np.mean(scores)
    
    def _analyze_search_results(self):
        """Analyze grid search results"""
        results_df = pd.DataFrame(self.search_results)
        
        # Sort by score
        results_df = results_df.sort_values('avg_score')
        
        # Log top 5
        logger.info("Top 5 weight combinations:")
        for _, row in results_df.head(5).iterrows():
            logger.info(f"Score: {row['avg_score']:.4f} Â± {row['std_score']:.4f}, "
                       f"Weights: {row['weights']}")
        
        # Check for patterns
        # Are certain ranges consistently better?
        for param in ['games_1_2', 'games_3_5', 'games_6_10', 'games_11_plus']:
            values = [r['weights'][param] for r in self.search_results]
            scores = [r['avg_score'] for r in self.search_results]
            
            # Group by parameter value
            param_performance = {}
            for val, score in zip(values, scores):
                if val not in param_performance:
                    param_performance[val] = []
                param_performance[val].append(score)
            
            # Average performance by value
            avg_by_value = {
                val: np.mean(scores) 
                for val, scores in param_performance.items()
            }
            
            best_val = min(avg_by_value, key=avg_by_value.get)
            logger.info(f"Best value for {param}: {best_val}")
```
**Win**: Systematic weight optimization with cross-validation

### Step 13: Bayesian Optimization
```python
# shl/optimization/bayesian_opt.py
from skopt import gp_minimize
from skopt.space import Real
from skopt.utils import use_named_args
import numpy as np
import structlog

logger = structlog.get_logger()

class BayesianWeightOptimizer:
    def __init__(self, config: Dict):
        self.config = config
        self.optimization_history = []
        
    def optimize_weights_bayesian(self,
                                historical_data: pd.DataFrame,
                                odds_data: Dict,
                                n_calls: int = 50,
                                n_initial_points: int = 10) -> Dict[str, float]:
        """Bayesian optimization for weight schedules"""
        
        # Define search space with constraints
        dimensions = [
            Real(0.5, 0.85, name='games_1_2'),
            Real(0.3, 0.7, name='games_3_5'),
            Real(0.1, 0.5, name='games_6_10'),
            Real(0.05, 0.3, name='games_11_plus')
        ]
        
        # Split data for validation
        train_size = int(len(historical_data) * 0.8)
        train_data = historical_data[:train_size]
        val_data = historical_data[train_size:]
        
        @use_named_args(dimensions)
        def objective(**params):
            # Check constraint: weights should decrease
            weights = [params['games_1_2'], params['games_3_5'], 
                      params['games_6_10'], params['games_11_plus']]
            
            if not all(weights[i] > weights[i+1] for i in range(3)):
                # Penalize invalid combinations
                return 1.0
            
            # Evaluate these weights
            score = self._evaluate_weights_cv(
                params, train_data, val_data, odds_data
            )
            
            # Log progress
            self.optimization_history.append({
                'iteration': len(self.optimization_history),
                'weights': params,
                'score': score
            })
            
            if len(self.optimization_history) % 10 == 0:
                logger.info(f"Iteration {len(self.optimization_history)}: "
                           f"Score = {score:.4f}")
            
            return score
        
        # Run optimization
        logger.info("Starting Bayesian optimization...")
        result = gp_minimize(
            func=objective,
            dimensions=dimensions,
            n_calls=n_calls,
            n_initial_points=n_initial_points,
            random_state=42,
            acq_func='EI'  # Expected Improvement
        )
        
        # Extract best weights
        best_weights = {
            'games_1_2': result.x[0],
            'games_3_5': result.x[1],
            'games_6_10': result.x[2],
            'games_11_plus': result.x[3]
        }
        
        logger.info(f"Optimization complete. Best score: {result.fun:.4f}")
        logger.info(f"Best weights: {best_weights}")
        
        # Analyze convergence
        self._analyze_convergence()
        
        return best_weights
    
    def _evaluate_weights_cv(self, weights: Dict, train_data: pd.DataFrame,
                           val_data: pd.DataFrame, odds_data: Dict) -> float:
        """Evaluate with multiple metrics"""
        evaluator = WeightEvaluator(weights)
        
        # Get predictions
        predictions = evaluator.predict_matches(val_data, odds_data)
        
        # Calculate multiple metrics
        metrics = {
            'brier': evaluator.calculate_brier_score(predictions),
            'log_loss': evaluator.calculate_log_loss(predictions),
            'accuracy': evaluator.calculate_accuracy(predictions)
        }
        
        # Weighted combination (lower is better)
        combined_score = (
            0.5 * metrics['brier'] + 
            0.3 * metrics['log_loss'] + 
            0.2 * (1 - metrics['accuracy'])  # Convert to loss
        )
        
        return combined_score
    
    def _analyze_convergence(self):
        """Analyze optimization convergence"""
        if len(self.optimization_history) < 10:
            return
        
        scores = [h['score'] for h in self.optimization_history]
        
        # Check if converged
        recent_scores = scores[-10:]
        if np.std(recent_scores) < 0.001:
            logger.info("Optimization converged")
        
        # Find iteration with best score
        best_iter = np.argmin(scores)
        best_score = scores[best_iter]
        best_weights = self.optimization_history[best_iter]['weights']
        
        logger.info(f"Best iteration: {best_iter} with score {best_score:.4f}")
        
        # Plot convergence (if in notebook/dashboard)
        self._create_convergence_plot(scores)
    
    def _create_convergence_plot(self, scores: List[float]):
        """Create convergence visualization"""
        import plotly.graph_objects as go
        
        fig = go.Figure()
        
        # Add optimization trace
        fig.add_trace(go.Scatter(
            x=list(range(len(scores))),
            y=scores,
            mode='lines+markers',
            name='Objective Value',
            line=dict(color='blue', width=2),
            marker=dict(size=6)
        ))
        
        # Add best value line
        best_score = min(scores)
        fig.add_hline(y=best_score, line_dash="dash", 
                     annotation_text=f"Best: {best_score:.4f}")
        
        fig.update_layout(
            title="Bayesian Optimization Convergence",
            xaxis_title="Iteration",
            yaxis_title="Objective Value (lower is better)",
            showlegend=True
        )
        
        # Save or display
        fig.write_html("optimization_convergence.html")
```
**Win**: Efficient Bayesian optimization with convergence analysis

## Phase 6: Advanced Features (Hard)

### Step 14: Probability Regularization
```python
# shl/models/regularization.py
def regularize_extreme_probabilities(probs, shrinkage_factor=0.1):
    """Apply Bayesian shrinkage to extreme probabilities"""
    # Define prior (uniform)
    prior = [1/3, 1/3, 1/3]
    
    # Shrink toward prior for extreme values
    regularized = []
    for i, p in enumerate(probs):
        if p > 0.7:
            # Shrink extreme probabilities
            p_reg = (1 - shrinkage_factor) * p + shrinkage_factor * prior[i]
        else:
            p_reg = p
        regularized.append(p_reg)
    
    # Renormalize
    total = sum(regularized)
    return [p/total for p in regularized]
```
**Win**: More realistic probabilities, less overconfidence

### Step 15: Complete Evaluation Suite
```python
# shl/evaluation/metrics.py
def comprehensive_evaluation(predictions, actuals):
    """Calculate all relevant metrics"""
    metrics = {}
    
    # Accuracy
    correct = sum(1 for p, a in zip(predictions, actuals) 
                  if p['predicted'] == a['actual'])
    metrics['accuracy'] = correct / len(predictions)
    
    # Brier Score
    brier_scores = []
    for p, a in zip(predictions, actuals):
        outcome = [0, 0, 0]
        outcome[a['actual']] = 1
        brier = sum((p['probs'][i] - outcome[i])**2 for i in range(3))
        brier_scores.append(brier)
    metrics['brier_score'] = np.mean(brier_scores)
    
    # Log Loss
    log_losses = []
    for p, a in zip(predictions, actuals):
        actual_prob = p['probs'][a['actual']]
        log_losses.append(-np.log(max(actual_prob, 1e-15)))
    metrics['log_loss'] = np.mean(log_losses)
    
    # Calibration
    metrics['calibration'] = calculate_calibration_score(predictions, actuals)
    
    return metrics
```
**Win**: Complete picture of model performance

### Step 16: Production Monitoring Dashboard with Alerts
```python
# shl/monitoring/dashboard.py
import streamlit as st
from datetime import datetime, timedelta
import pandas as pd
from src.monitoring.alerts import check_model_drift

def create_monitoring_page():
    st.title("ðŸŽ¯ Model Performance Monitoring")
    
    # Check for alerts
    alerts = check_model_drift()
    if alerts:
        for alert in alerts:
            st.error(f"âš ï¸ {alert}")
    
    # Load recent predictions and results
    recent = load_recent_predictions(days=30)
    
    # Real-time metrics with trend indicators
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        accuracy = calculate_rolling_accuracy(recent, window=50)
        prev_accuracy = calculate_rolling_accuracy(recent, window=50, offset=7)
        delta = accuracy - prev_accuracy
        st.metric("Rolling Accuracy", f"{accuracy:.1%}", f"{delta:+.1%}")
    
    with col2:
        brier = calculate_rolling_brier(recent, window=50)
        prev_brier = calculate_rolling_brier(recent, window=50, offset=7)
        delta = brier - prev_brier
        st.metric("Rolling Brier Score", f"{brier:.3f}", f"{delta:+.3f}", delta_color="inverse")
    
    with col3:
        roi = calculate_roi_if_betting(recent)
        st.metric("ROI (if betting)", f"{roi:+.1%}", 
                 help="ROI if betting on model's highest confidence picks")
    
    with col4:
        calibration = calculate_calibration_score(recent)
        st.metric("Calibration Score", f"{calibration:.3f}",
                 help="How well predicted probabilities match actual frequencies")
    
    # Calibration plot with confidence bands
    st.subheader("ðŸ“Š Calibration Plot")
    fig = create_enhanced_calibration_plot(recent)
    st.plotly_chart(fig, use_container_width=True)
    
    # Performance breakdown
    st.subheader("ðŸ” Performance Analysis")
    
    tab1, tab2, tab3 = st.tabs(["By Confidence", "By Team", "By Time"])
    
    with tab1:
        perf_by_conf = analyze_by_confidence(recent)
        st.dataframe(
            perf_by_conf.style.background_gradient(subset=['Accuracy', 'Brier_Score']),
            use_container_width=True
        )
    
    with tab2:
        team_perf = analyze_by_team(recent)
        fig_team = create_team_performance_chart(team_perf)
        st.plotly_chart(fig_team, use_container_width=True)
    
    with tab3:
        time_perf = analyze_performance_over_time(recent)
        fig_time = create_time_series_chart(time_perf)
        st.plotly_chart(fig_time, use_container_width=True)
    
    # Model diagnostics
    with st.expander("ðŸ”§ Model Diagnostics"):
        st.subheader("Weight Analysis")
        weight_analysis = analyze_weight_effectiveness(recent)
        st.json(weight_analysis)
        
        st.subheader("Error Analysis")
        error_patterns = analyze_error_patterns(recent)
        st.dataframe(error_patterns)

# shl/monitoring/alerts.py
def check_model_drift():
    """Check for model performance degradation"""
    alerts = []
    
    # Load recent performance
    recent_7d = load_recent_predictions(days=7)
    recent_30d = load_recent_predictions(days=30)
    
    # Check accuracy drift
    acc_7d = calculate_accuracy(recent_7d)
    acc_30d = calculate_accuracy(recent_30d)
    
    if acc_7d < acc_30d * 0.9:  # 10% degradation
        alerts.append(f"Model accuracy dropped from {acc_30d:.1%} to {acc_7d:.1%}")
    
    # Check calibration drift
    cal_7d = calculate_calibration_score(recent_7d)
    cal_30d = calculate_calibration_score(recent_30d)
    
    if cal_7d > cal_30d * 1.2:  # 20% worse calibration
        alerts.append(f"Model calibration degraded from {cal_30d:.3f} to {cal_7d:.3f}")
    
    return alerts
```
**Win**: Comprehensive monitoring with automated alerts

## Summary: Updated Implementation Timeline

### Week 1: Foundation with Robustness
- **Day 1-2**: Schema validation, error handling, logging setup (Steps 1-3)
- **Day 3-4**: API abstraction, caching layer, config management
- **Day 5**: Unit tests, CI/CD pipeline setup

### Week 2: Core Integration  
- **Day 6-7**: Robust hybrid model with validation (Steps 4-5)
- **Day 8-9**: Parallelized Monte Carlo simulation (Step 6)
- **Day 10**: Integration tests, performance profiling

### Week 3: Advanced Features
- **Day 11-12**: Team strength extraction with caching (Steps 7-8)
- **Day 13-14**: Dynamic parameters (volatility, lookback) (Steps 9-11)
- **Day 15**: End-to-end testing with historical data

### Week 4: Optimization & Production
- **Day 16-17**: Grid search and Bayesian optimization (Steps 12-13)
- **Day 18-19**: Advanced features (regularization, evaluation) (Steps 14-15)
- **Day 20-21**: Production monitoring dashboard (Step 16)

### Week 5: Buffer & Polish
- **Day 22-23**: Bug fixes, edge case handling
- **Day 24-25**: Documentation, deployment guides
- **Day 26-27**: Performance optimization
- **Day 28**: Final testing and release

**Total Timeline**: 4 weeks development + 1 week buffer = 5 weeks

## Key Improvements Added:
1. **Data validation** with Pydantic schemas
2. **Comprehensive logging** with structlog
3. **API abstraction** for easy source switching
4. **Redis caching** for API calls
5. **Parallel simulation** with joblib
6. **Configuration management** via YAML
7. **Automated testing** framework
8. **Production monitoring** with alerts
9. **Error handling** at every boundary
10. **Performance profiling** hooks